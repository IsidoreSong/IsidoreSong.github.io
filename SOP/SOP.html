<!DOCTYPE html> 
<html lang='en-US' xml:lang='en-US'> 
<head><title></title> 
<meta charset='utf-8' /> 
<meta content='TeX4ht (https://tug.org/tex4ht/)' name='generator' /> 
<meta content='width=device-width,initial-scale=1' name='viewport' /> 
<link href='SOP.css' rel='stylesheet' type='text/css' /> 
<meta content='SOP.tex' name='src' /> 
</head><body>
<!-- l. 71 --><p class='noindent'></p><div class='minipage'><span class='bchb8t-x-x-109'>STATEMENT
OF
PURPOSE</span><br />                                             </div><div class='minipage'>                                                               <span class='bchb8t-x-x-109'>Youzhe
                                                                  Song</span><br />
                                                                       <a href='https://isidoresong.github.io/'><img alt='x
                                               ' src='SOP0x.png' /><span class='bchr8t-'> Homepage</span></a><br />
                                                                    Computer
                                                                  Science
                                                                       PhD
                                                              Applicant,
                                                                        Fall
                                                                      2026</div>
<h3 class='sectionHead' id='introduction-my-research-interests-and-perspective'><span class='titlemark'>I    </span> <a id='x1-1000I'></a>Introduction: My Research Interests and Perspective</h3>
<!-- l. 89 --><p class='noindent'>Current AI models have achieved remarkable success across tasks, yet their internal mechanisms
remain largely a ~black box.~ We know what models can do, and to some extent how they do it
in specific details, but we lack an intuitively human-understandable view of their cognitive
and reasoning processes. I believe that a key step toward more general and reliable AI lies in
understanding and designing the model’s internal information-processing mechanisms. My
research interests center on two core questions: First, can we construct within the model a
<span class='bchb8t-x-x-109'>human-intuitive, stepwise, and organic reasoning trajectory</span>? Second, how can we enable the model to
<span class='bchb8t-x-x-109'>understand and align information from different sources </span>(e.g., images and text) in a human-like
manner?
</p><!-- l. 91 --><p class='noindent'>My goal is to explore how to realize the fusion of these two abilities in a high-dimensional latent space,
enabling models not only to ~see~ and ~read~ the world, but also to reason about it in a transparent and
trustworthy way.
</p><!-- l. 93 --><p class='noindent'>
</p>
<h3 class='sectionHead' id='robust-feature-representations-coreface'><span class='titlemark'>II    </span> <a id='x1-2000II'></a>Robust Feature Representations (CoReFace)</h3>
<!-- l. 95 --><p class='noindent'>My research journey began with a deep fascination for self-supervised contrastive learning. It does
not rely on expensive human annotations; instead, it learns from the structure inherent in
data. Its elegance and potential captivated me. However, the realities of a lab with very limited
compute and diverse student directions pushed me to give it up and pursue a more fundamental
alternative.
</p><!-- l. 97 --><p class='noindent'>I found face recognition to be an ideal ~sandbox.~ I observed its deep kinship with contrastive learning:
both carefully shape the geometry of feature distributions in high-dimensional space and share the same
goal the to be generalized in open-set. Building on this, I proposed the CoReFace framework, which is an
instance of explicit design of internal mechanisms. In this fine-grained recognition task, traditional image
augmentations can damage identity information. A key insight I had was that Dropout is essentially a
                                                                                          
                                                                                          
<span class='bchb8t-x-x-109'>feature-level stochastic perturbation </span>that can provide the necessary views for contrastive
learning without corrupting image semantics. After introducing contrastive learning, I further
enhanced the weak supervising signal to make it work and rearange the pair construction in
contrastive learning to avoid semantical repeat signal problem in the unified training. All of
these made the framework actively <span class='bchb8t-x-x-109'>regularized </span>the geometric structure of the feature space.
Ultimately, this approach <span class='bchb8t-x-x-109'>increased the similarity margin between positive and negative
pairs by 15%</span>. Throughout, I led the full lifecycle—from problem identification and design to
independent validation and writing—shifting from ~student~ to a truly <span class='bchb8t-x-x-109'>independent researcher
</span><span class='cite'>[<a href='#Xcoreface'>1</a>]</span>.
</p><!-- l. 99 --><p class='noindent'>
</p>
<h3 class='sectionHead' id='a-unified-framework-for-heterogeneous-data-qgface'><span class='titlemark'>III    </span> <a id='x1-3000III'></a>A Unified Framework for Heterogeneous Data (QGFace)</h3>
<!-- l. 101 --><p class='noindent'>After CoReFace, I yearned for research with more immediate real-world relevance. This was reinforced
when I scrolled through my phone’s photo album: the built-in face clustering struggled with family photos.
I realized that low-quality data arising from composition, lighting, and other real-world factors differ
markedly from our common datasets.
</p><!-- l. 103 --><p class='noindent'>QGFace’s <span class='bchb8t-x-x-109'>single-encoder architecture </span>is my solution. Its an internal <span class='bchb8t-x-x-109'>information dispatch system</span>. It
simulates a ~routing~ process: high-quality data follow a classification path, low-quality data follow a
contrastive path, and gradient truncation prevents contamination across paths. To learn to recognize
low-quality data, I applied augmentation on images this time. The first thing is to design the
augmentation pipeline to avoid model collapse during training. Later when contrastive learning
underperformed due to insufficient positive pairs, I designed a <span class='bchb8t-x-x-109'>Proxy-Updated Real-Time Queue </span>that
boosted performance on low-quality data. Racing against a deadline, I structured my days into
intense cycles; GPU time became my rest time. Far from being a burden, I felt little depression at
that time as I knew that this was something being built. It proved that what drives me is not
external expectation, but the intrinsic intellectual joy of solving hard problems. Ultimately,
QGFace reached SOTA on low-quality datasets with <span class='bchb8t-x-x-109'>only a 0.3% trade-off </span>on high-quality
data, validating that my research philosophy can yield elegant, robust, and practical systems
<span class='cite'>[<a href='#Xqgface'>2</a>]</span>.
</p><!-- l. 106 --><p class='noindent'>
</p>
<h3 class='sectionHead' id='exploration-clarifying-my-direction-through-working-practice'><span class='titlemark'>IV    </span> <a id='x1-4000IV'></a>Exploration: Clarifying My Direction Through Working Practice</h3>
<!-- l. 108 --><p class='noindent'>Despite some academic progress, repeated setbacks and uncertainty in the submission process led to a
period of deep confusion and self-doubt. To find clarity, I decided to step into industry and <span class='bchb8t-x-x-109'>stress-test </span>my
intrinsic motivation.
</p><!-- l. 110 --><p class='noindent'>I first joined KeyoneAI, a startup led by former IBM China Chief AI Architect Jie Fang. There, I translated
frontier generative AI into product, felt the pulse of rapid iteration, advocated technology, and engaged
with users.
                                                                                          
                                                                                          
</p><!-- l. 112 --><p class='noindent'>Later, I served as the sole technical lead on the organizing committee of the Worldwide Educators
Conference (WWEC) <span class='cite'>[<a href='#Xwwec'>3</a>]</span>. Beyond ensuring system reliability, I accelerated the team’s digital
transformation, developed internal tools to generate 1000+ complex posters, and connected 3,000
attendees, 1,000 exhibitors, 10+ vendors, and many ad-hoc sessions—working 80 hours per week for
nearly three months. While challenging and rewarding, these experiences still could not replace the pure
intellectual excitement and flow I feel in research, witnessing breakthroughs in a field and
contributing to them. This deliberate detour granted me unprecedented clarity: my deepest drive is to
question fundamentals, refute and rebuild existing solutions, and innovate effectively across
disciplines.
</p><!-- l. 114 --><p class='noindent'>More importantly, this exploration helped me reframe what used to be excessive self-scrutiny into a unique
research lens. I understand deeply that <span class='bchb8t-x-x-109'>a truly intelligent system is not defined by flawless
unidirectional reasoning but by its capacity to handle internal conflict, self-examination, and
iterative revision</span>—the essence of human reflection and ~productive struggle.~
</p><!-- l. 116 --><p class='noindent'>
</p>
<h3 class='sectionHead' id='future-align-semantics-reason-in-latent-space'><span class='titlemark'>V    </span> <a id='x1-5000V'></a>Future: Align Semantics &amp; Reason in Latent Space</h3>
<!-- l. 118 --><p class='noindent'>I now plan my Ph.D. research with renewed clarity and conviction. I aim to combine my experience in
<span class='bchb8t-x-x-109'>mechanism design </span>with reflections on <span class='bchb8t-x-x-109'>human cognition</span>, focusing on two capabilities of large
models:
</p><!-- l. 120 --><p class='noindent'><span class='bchb8t-x-x-109'>1. Building a Unified Semantic Space: </span>My first goal is to study how to effectively map diverse information
into a unified, interpretable latent space. This goes beyond cross-modal mapping. It is the lower code of
how model understand the world laws, like physics. I believe that efficient cross-modal semantic
alignment is a first step toward foundational models that can comprehensively understand the
world.
</p><!-- l. 122 --><p class='noindent'><span class='bchb8t-x-x-109'>2. Realizing Structured Reasoning in Latent Space: </span>The other goal is to design <span class='bchri8t-x-x-109'>structured</span>,
chain-of-thought-like reasoning trajectories within the latent space. I want models not only to be
like the early computers that used punched paper tape for data input and output (which is
transformer doing), but also to present a decomposable and traceable reasoning process in
itself.
</p><!-- l. 124 --><p class='noindent'>In summary, my research proceeds on two fronts: through <span class='bchb8t-x-x-109'>sementic alignment</span>, enabling models to
~see~ a richer world; and through <span class='bchb8t-x-x-109'>latent reasoning</span>, enabling them to ~think~ more clearly and
logically.
</p><!-- l. 126 --><p class='noindent'>
</p>
<h3 class='sectionHead' id='conclusion'><span class='titlemark'>VI    </span> <a id='x1-6000VI'></a>Conclusion</h3>
<!-- l. 128 --><p class='noindent'>With hands-on experience designing internal mechanisms and a clear plan to integrate <span class='bchri8t-x-x-109'>semantic alignment</span>
with <span class='bchri8t-x-x-109'>latent reasoning</span>, I am prepared for the challenges of a Ph.D. I look forward to contributing to
                                                                                          
                                                                                          
the next generation of more capable and trustworthy AI systems in a creative and supportive
environment.
</p><!-- l. 1 --><p class='noindent'>
</p>
<h3 class='likesectionHead' id='references'><a id='x1-7000'></a>References</h3>
<!-- l. 1 --><p class='noindent'>
    </p><div class='thebibliography'>
    <p class='bibitem'><span class='biblabel'>
 [1]<span class='bibsp'>   </span></span>
    <a id='Xcoreface'></a>Youzhe
    Song
    and
    Feng
    Wang.
    Coreface:
    Sample-guided
    contrastive
    regularization
    for
    deep
    face
    recognition.
    <span class='bchri8t-x-x-109'>Pattern
    Recognition</span>,
    152:110483,
    2024.
    </p>
    <p class='bibitem'><span class='biblabel'>
 [2]<span class='bibsp'>   </span></span>
    <a id='Xqgface'></a>Youzhe
    Song
    and
    Feng
    Wang.
    Quality-guided
    joint
    training
    for
    mixed-quality
    face
                                                                                          
                                                                                          
    recognition.
    In
    <span class='bchri8t-x-x-109'>2024
    IEEE
    International
    Conference
    on
    Automatic
    Face
    and
    Gesture
    Recognition
    (FG)</span>.
    IEEE,
    2024.
    </p>
    <p class='bibitem'><span class='biblabel'>
 [3]<span class='bibsp'>   </span></span>
    <a id='Xwwec'></a>Worldwide
    educators
    conference
    (wwec).
    
<a class='url' href='https://www.wwec820.com/'><span class='ectt-1095'>https://www.wwec820.com/</span></a>.
</p>
    </div>
 
</body> 
</html>